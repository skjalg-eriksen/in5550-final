{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\\Users\\skjal\\Code\\Master\\in5550\\final\\in5550-final\n"
     ]
    }
   ],
   "source": [
    "# go to source directory for this project\n",
    "%cd A:\\Users\\skjal\\Code\\Master\\in5550\\final\\in5550-final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\Users\\skjal\\Miniconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchtext import data, vocab\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from helpers import *\n",
    "from Models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def main():\n",
    "    #torch.manual_seed(1337)\n",
    "    #random.seed(13370)\n",
    "\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument('--train', action='store')\n",
    "    #parser.add_argument('--dev', action='store')\n",
    "    #parser.add_argument('--embeds', action='store')\n",
    "    #args = parser.parse_args()\n",
    "    \n",
    "args = argparse.Namespace()\n",
    "args.train = \"nli5550/nli5550.train.jsonl\"\n",
    "args.dev = \"nli5550/nli5550.dev.jsonl\"\n",
    "args.test = \"nli5550/nli5550.test.jsonl\"\n",
    "args.embeds = \"../model.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_field = data.Field(sequential=True, batch_first=True, include_lengths=True, tokenize=lambda x: x.split(), preprocessing=lambda x: x[1].split())\n",
    "label_field = data.Field(sequential=False, batch_first=True, preprocessing=lambda x: x[1])\n",
    "NaF = ('none', None)\n",
    "\n",
    "fields = [\n",
    "            NaF, #('annotator_labels', label_field), \n",
    "            NaF, # captionID\n",
    "            ('gold_label', label_field), \n",
    "            NaF, # pairID\n",
    "            NaF, #('sentence1', token_field),\n",
    "            NaF, #('sentence1_parse', token_field),\n",
    "            NaF, #('sentence2', token_field),\n",
    "            NaF, #('sentence2_parse', token_field),\n",
    "            ('sentence1_tok', token_field),\n",
    "            ('sentence2_tok', token_field)\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_data = False\n",
    "if split_train_data:\n",
    "    train_dataset = data.Dataset( load_jsonl_examples(args.train, fields) , fields=fields).split(0.2)[0]\n",
    "else:\n",
    "    train_dataset = data.Dataset( load_jsonl_examples(args.train, fields) , fields=fields)\n",
    "dev_dataset = data.Dataset( load_jsonl_examples(args.dev, fields) , fields=fields)\n",
    "test_dataset = data.Dataset( load_jsonl_examples(args.test, fields) , fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset lenght:\t550152\n",
      "dev_dataset lenght:\t10000\n",
      "test_dataset lenght:\t10000\n"
     ]
    }
   ],
   "source": [
    "print( 'train_dataset lenght:\\t{}'.format(len(train_dataset)))\n",
    "print( 'dev_dataset lenght:\\t{}'.format(len(dev_dataset)))\n",
    "print( 'test_dataset lenght:\\t{}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: build vocabs\n",
    "#_vecs = vocab.Vectors(args.embeds)\n",
    "_vecs = \"glove.6B.100d\"\n",
    "use_test = True\n",
    "if use_test:\n",
    "    token_field.build_vocab(test_dataset, vectors=_vecs)\n",
    "    label_field.build_vocab(test_dataset)\n",
    "    train_iter = data.Iterator(test_dataset, batch_size=32, train=True, sort=True, repeat=False, sort_key=lambda x: (len(x.sentence1_tok)+len(x.sentence1_tok))/2)\n",
    "else:\n",
    "    token_field.build_vocab(train_dataset, vectors=_vecs)\n",
    "    label_field.build_vocab(train_dataset)\n",
    "    train_iter = data.Iterator(train_dataset, batch_size=32, train=True, sort=True, repeat=False, sort_key=lambda x: (len(x.sentence1_tok)+len(x.sentence1_tok))/2)\n",
    "\n",
    "dev_iter = data.Iterator(dev_dataset, batch_size=1, train=False, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class noteboookmodel2(nn.Module):\n",
    "    def __init__(self, token_vocab, tag_vocab):\n",
    "        super().__init__()\n",
    "        \n",
    "        input_size = token_vocab.vectors.shape[1]*16 # output size of the encoder\n",
    "        hidden_state_size = 512\n",
    "        #self._embeds = nn.Embedding.from_pretrained(token_vocab.vectors)\n",
    "        self._embeds = nn.Embedding(len(token_vocab), 300, padding_idx=token_vocab.stoi['<pad>']).from_pretrained(token_vocab.vectors)\n",
    "        # define the rest of your components\n",
    "        self._conv1 = nn.Conv1d(in_channels=token_vocab.vectors.shape[1],\n",
    "                                out_channels=token_vocab.vectors.shape[1],\n",
    "                                kernel_size=3, \n",
    "                                padding=1)\n",
    "        self._conv2 = nn.Conv1d(in_channels=token_vocab.vectors.shape[1],\n",
    "                                out_channels=token_vocab.vectors.shape[1],\n",
    "                                kernel_size=3, \n",
    "                                padding=1)\n",
    "        self._conv3 = nn.Conv1d(in_channels=token_vocab.vectors.shape[1],\n",
    "                                out_channels=token_vocab.vectors.shape[1],\n",
    "                                kernel_size=3, \n",
    "                                padding=1)\n",
    "        self._conv4 = nn.Conv1d(in_channels=token_vocab.vectors.shape[1],\n",
    "                                out_channels=token_vocab.vectors.shape[1],\n",
    "                                kernel_size=3, \n",
    "                                padding=1)\n",
    "\n",
    "        self.MLP = nn.Sequential(nn.Linear(input_size, hidden_state_size), # input layer\n",
    "                                 nn.ReLU(), # hidden layer\n",
    "                                 nn.Linear(hidden_state_size, len(tag_vocab))) # ouput layer\n",
    "    def forward(self, batch):\n",
    "        verbose = False\n",
    "        #tokens, lengths = batch\n",
    "        tokens = [batch.sentence1_tok, batch.sentence2_tok]\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            token, lengths = token\n",
    "            \n",
    "            # embeddings go here\n",
    "            embeds = self._embeds(token).transpose(1, 2)\n",
    "            if verbose: print(\"embeds\",embeds.shape)\n",
    "            # conv + pooling\n",
    "            c1 = self._conv1(embeds)\n",
    "            if verbose: print(\"c1\",c1.shape)\n",
    "            c2 = self._conv2(c1)\n",
    "            if verbose: print(\"c2\",c2.shape)\n",
    "            c3 = self._conv3(c2)\n",
    "            if verbose: print(\"c3\",c3.shape)\n",
    "            c4 = self._conv4(c2)\n",
    "            if verbose: print(\"c4\",c4.shape)\n",
    "            #tokens[i] = F.dropout(torch.cat([c1.max(dim=-1)[0], c2.max(dim=-1)[0], c3.max(dim=-1)[0], c4.max(dim=-1)[0]], dim=1), p=0.33, training=self.training)\n",
    "            tokens[i] = torch.cat([c1.max(dim=-1)[0], c2.max(dim=-1)[0], c3.max(dim=-1)[0], c4.max(dim=-1)[0]], dim=1)\n",
    "            if verbose: print(\"sentence {}\".format(i), tokens[i].shape)\n",
    "        \n",
    "        u, v = tokens\n",
    "        \n",
    "        # [u,v |u-v|, u*v]\n",
    "        a = torch.cat([u, v], dim=1)\n",
    "        b = torch.abs(u-v)\n",
    "        c = u*v\n",
    "        vec = torch.cat([a, b, c], dim=1)\n",
    "        if verbose: print('vec', vec.shape)\n",
    "        return self.MLP(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(model, iterator, criterion, optimiser, epoch, writer=None):\n",
    "    \n",
    "    model.train()\n",
    "    #encoder.train()\n",
    "    #pbar = tqdm(total=len(iterator.dataset))\n",
    "    pbar = tqdm(total=len(iterator.data()))\n",
    "    accuracy, total = 0, 0\n",
    "    total_loss = 0\n",
    "    iterator.init_epoch()\n",
    "    for n, batch in enumerate(iterator):\n",
    "        optimiser.zero_grad()\n",
    "        #u = encoder(batch.sentence1_tok)\n",
    "        #v = encoder(batch.sentence2_tok)\n",
    "\n",
    "        predictions = model(batch)\n",
    "        gold = batch.gold_label\n",
    "        \n",
    "        loss = criterion(predictions, gold)\n",
    "        \n",
    "        predmax = predictions.argmax(dim=-1)\n",
    "        total_loss += loss.item()\n",
    "        accuracy += ( predmax== gold).nonzero().size(0)\n",
    "        total += predmax.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        pbar.update(32) # batch_size\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "        if writer is not None: writer.add_scalar('Train/Loss', loss, n)\n",
    "        \n",
    "    if writer is not None: writer.add_scalar('Train/total_loss', total_loss, epoch)\n",
    "    if writer is not None: writer.add_scalar('Train/mean_loss', total_loss/total, epoch)\n",
    "    if writer is not None: writer.add_scalar('Train/accuracy', accuracy/total, epoch)\n",
    "    \n",
    "    pbar.close()\n",
    "\n",
    "def evaluate(model, iterator, criterion, epoch, writer=None):\n",
    "    model.eval()\n",
    "    accuracy, total = 0, 0\n",
    "    total_loss = 0\n",
    "    for n, batch in enumerate(iterator):\n",
    "        prediction = model(batch)\n",
    "        predictions = prediction.argmax(dim=-1)\n",
    "        gold = batch.gold_label\n",
    "        loss = criterion(prediction, gold)\n",
    "        total_loss += loss\n",
    "        accuracy += (predictions == gold).nonzero().size(0)\n",
    "        total += predictions.size(0)\n",
    "        if writer is not None: writer.add_scalar('Dev/Loss', loss, n)\n",
    "    \n",
    "    if writer is not None: writer.add_scalar('Dev/total_loss', total_loss, epoch)\n",
    "    if writer is not None: writer.add_scalar('Dev/mean_loss', total_loss/total, epoch)\n",
    "    if writer is not None: writer.add_scalar('Dev/accuracy', accuracy/total, epoch)\n",
    "    print(\"> dev accuracy: {}/{} = {}\".format(accuracy, total, accuracy/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "class self_attention_Encoder(nn.Module):\n",
    "    def __init__(self, token_vocab, hidden_size=1024, dimension_a=100, attention_hops=4):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # word embeddings\n",
    "        #self.embedding = nn.Embedding.from_pretrained(token_vocab.vectors)\n",
    "        self.embedding = nn.Embedding(len(token_vocab), 300, padding_idx=token_vocab.stoi['<pad>']).from_pretrained(token_vocab.vectors)\n",
    "\n",
    "        # to achive bi-direction i stack 2 LSTMCells, one for each direction; RNN_1 (->), RNN_2 (<-)\n",
    "        self.RNN_1 = nn.LSTMCell(token_vocab.vectors.shape[1], hidden_size, bias=True)\n",
    "        self.RNN_2 = nn.LSTMCell(token_vocab.vectors.shape[1], hidden_size, bias=True)\n",
    "        \n",
    "        #self.RNN_1 = nn.RNNCell(token_vocab.vectors.shape[1], hidden_size, bias=True)\n",
    "        #self.RNN_2 = nn.RNNCell(token_vocab.vectors.shape[1], hidden_size, bias=True)\n",
    "        \n",
    "        \n",
    "        # alpha weights\n",
    "        Ws1 = torch.zeros(dimension_a, hidden_size*2)\n",
    "        nn.init.normal_(Ws1, mean=0.0, std=1.0)\n",
    "        self.Ws1 = nn.Parameter(Ws1)\n",
    "        \n",
    "        Ws2 = torch.zeros(attention_hops, dimension_a)\n",
    "        nn.init.normal_(Ws2, mean=1.0, std=1.0)\n",
    "        self.Ws2 = nn.Parameter(Ws2)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        tokens, _ = sentence;\n",
    "        batch_size = len(_)\n",
    "        H_1 = [] # layer 1, ->\n",
    "        H_2 = [] # layer 2, <-\n",
    "        A = [] # softmax(ws2*tanh(ws1*Ht))\n",
    "        \n",
    "        # get embedding vectors\n",
    "        embedded = self.embedding(tokens)\n",
    "        embedded = embedded.transpose(1,0)\n",
    "     \n",
    "        \n",
    "        # initial word for layer 1\n",
    "        hidden_state = self.RNN_1(embedded[0])\n",
    "        H_1.append(hidden_state)\n",
    "        \n",
    "        # layer 1 RNN\n",
    "        for token in embedded[1:]:\n",
    "            hidden_state = self.RNN_1(token, hidden_state)\n",
    "            H_1.append(hidden_state)\n",
    "        \n",
    "        # flip the words to do layer 2\n",
    "        embedded = torch.flip(embedded, [0])\n",
    "        \n",
    "        # initial word for layer 2\n",
    "        hidden_state = self.RNN_2(embedded[0])\n",
    "        H_2.append(hidden_state)\n",
    "       \n",
    "        # layer 2 RNN\n",
    "        for token in embedded[1:]:\n",
    "            hidden_state = self.RNN_2(token, hidden_state)\n",
    "            H_2.append(hidden_state)\n",
    "       \n",
    "        #flip H_2 so the hidden states line up with H_1\n",
    "        H_2.reverse()\n",
    "            \n",
    "        # concat each hidden states for H_1, H_2\n",
    "        H = []\n",
    "        for h1, h2 in zip(H_1, H_2):\n",
    "            H.append(torch.cat([h1[0], h2[0]], dim=1))\n",
    "        H = torch.stack(H) # turn python list of tensors into 1 tensor for the hidden states\n",
    "        H = H.transpose(1,0)\n",
    "        \n",
    "        # for every sentence in batch calculate attention\n",
    "        for batched_H in H:\n",
    "            A.append(F.softmax(self.Ws2 @ F.tanh(self.Ws1 @ batched_H.transpose(0,1)), dim=1) )\n",
    "        \n",
    "        \n",
    "        # attention distrubution\n",
    "        A = torch.stack(A)\n",
    "        u = A@H # weighted sums\n",
    "        u = u.view((batch_size, -1, 1))\n",
    "        u = torch.squeeze(u, -1)\n",
    "        return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_Encoder3(nn.Module):\n",
    "    def __init__(self, token_vocab, hidden_size=1024, dimension_a=100, attention_hops=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # word embeddings\n",
    "        #self.embedding = nn.Embedding.from_pretrained(token_vocab.vectors)\n",
    "        self.embedding = nn.Embedding(len(token_vocab), 300, padding_idx=token_vocab.stoi['<pad>']).from_pretrained(token_vocab.vectors)\n",
    "\n",
    "        # to achive bi-direction i stack 2 LSTMCells, one for each direction; RNN_1 (->), RNN_2 (<-)\n",
    "        self.RNN_1 = nn.LSTMCell(token_vocab.vectors.shape[1], hidden_size, bias=True)\n",
    "        self.RNN_2 = nn.LSTMCell(token_vocab.vectors.shape[1], hidden_size, bias=True)\n",
    "        \n",
    "        #self.RNN_1 = nn.RNNCell(token_vocab.vectors.shape[1], hidden_size, bias=True)\n",
    "        #self.RNN_2 = nn.RNNCell(token_vocab.vectors.shape[1], hidden_size, bias=True)\n",
    "        \n",
    "        \n",
    "        # alpha weights\n",
    "        Ws1 = torch.zeros(dimension_a, hidden_size*2)\n",
    "        nn.init.normal_(Ws1, mean=0.0, std=1.0)\n",
    "        self.Ws1 = nn.Parameter(Ws1)\n",
    "        \n",
    "        Ws2 = torch.zeros(attention_hops, dimension_a)\n",
    "        nn.init.normal_(Ws2, mean=1.0, std=1.0)\n",
    "        self.Ws2 = nn.Parameter(Ws2)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        tokens, _ = sentence;\n",
    "        batch_size = len(_)\n",
    "        H_1 = [] # layer 1, ->\n",
    "        H_2 = [] # layer 2, <-\n",
    "        A = [] # softmax(ws2*tanh(ws1*Ht))\n",
    "        \n",
    "        # get embedding vectors\n",
    "        embedded = self.embedding(tokens)\n",
    "        embedded = embedded.transpose(1,0)\n",
    "     \n",
    "        \n",
    "        # initial word for layer 1\n",
    "        hidden_state = self.RNN_1(embedded[0])\n",
    "        H_1.append(hidden_state)\n",
    "        \n",
    "        # layer 1 RNN\n",
    "        for token in embedded[1:]:\n",
    "            hidden_state = self.RNN_1(token, hidden_state)\n",
    "            H_1.append(hidden_state)\n",
    "        \n",
    "        # flip the words to do layer 2\n",
    "        embedded = torch.flip(embedded, [0])\n",
    "        \n",
    "        # initial word for layer 2\n",
    "        hidden_state = self.RNN_2(embedded[0])\n",
    "        H_2.append(hidden_state)\n",
    "       \n",
    "        # layer 2 RNN\n",
    "        for token in embedded[1:]:\n",
    "            hidden_state = self.RNN_2(token, hidden_state)\n",
    "            H_2.append(hidden_state)\n",
    "       \n",
    "        #flip H_2 so the hidden states line up with H_1\n",
    "        H_2.reverse()\n",
    "        \n",
    "        # concat each hidden states for H_1, H_2\n",
    "        H = []\n",
    "        for h1, h2 in zip(H_1, H_2):\n",
    "            H.append(torch.cat([h1[0], h2[0]], dim=1))\n",
    "\n",
    "        H = torch.stack(H) # turn python list of tensors into 1 tensor for the hidden states\n",
    "        H = H.transpose(1,0)\n",
    "        \n",
    "        a = [ F.softmax(self.Ws2 @ F.tanh(self.Ws1 @ h.t()), dim=1) for h in H ]\n",
    "        \n",
    "        # attention distrubution\n",
    "        A = torch.stack(a)\n",
    "        #go through all in batch and do operation\n",
    "        v = []\n",
    "        for a, h in zip(A,H):\n",
    "            v.append(a@h)\n",
    "        u = torch.stack(v)\n",
    "        #print('u',u.shape)\n",
    "        u_ = [ torch.cat([v for v in u_], dim=-1) for u_ in u ]\n",
    "        u = torch.stack(u_)\n",
    "       \n",
    "        #print('U',u.shape)\n",
    "        # batch_size x attention_hops x hidden_size*2\n",
    "        return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, token_vocab, tag_vocab, encoder, input_size=1024, hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        input_size = input_size * 2 * 4 * 4\n",
    "        self.MLP = nn.Sequential(nn.Linear(input_size, hidden_size), # input layer\n",
    "                                 nn.ReLU(), # hidden layer\n",
    "                                 nn.Linear(hidden_size, len(tag_vocab))) # ouput layer\n",
    "    def forward(self, batch):\n",
    "        u = self.encoder(batch.sentence1_tok)\n",
    "        v = self.encoder(batch.sentence2_tok)\n",
    "        \n",
    "        # [u, v |u-v|, u*v]\n",
    "        vec = torch.cat([torch.cat([u, v], dim=1), torch.abs(u-v), u*v], dim=1)\n",
    "        \n",
    "        return self.MLP(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_Encoder2(nn.Module):\n",
    "    def __init__(self, token_vocab, hidden_size=1024, dimension_a=256, attention_hops=4):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # word embeddings\n",
    "        #self.embedding = nn.Embedding.from_pretrained(token_vocab.vectors)\n",
    "        self.embedding = nn.Embedding(len(token_vocab), 300, padding_idx=token_vocab.stoi['<pad>']).from_pretrained(token_vocab.vectors)\n",
    "\n",
    "        self.RNN = nn.LSTM(token_vocab.vectors.shape[1], hidden_size, bias=True, batch_first=True, bidirectional=True)\n",
    "        #self.RNN_2 = nn.LSTM(token_vocab.vectors.shape[1], hidden_size, bias=True, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.MLP = nn.Sequential(nn.Linear(hidden_size*2, dimension_a), # input layer\n",
    "                                 nn.Tanh(), # hidden layer\n",
    "                                 nn.Linear(dimension_a, attention_hops), # ouput layer\n",
    "                                 nn.Softmax(dim=1)\n",
    "                                )\n",
    "        # alpha weights\n",
    "        #Ws1 = torch.zeros(dimension_a, hidden_size*2)\n",
    "        #nn.init.normal_(Ws1, mean=0.0, std=1.0)\n",
    "        #self.Ws1 = nn.Parameter(Ws1)\n",
    "        \n",
    "        #Ws2 = torch.zeros(attention_hops, dimension_a)\n",
    "        #nn.init.normal_(Ws2, mean=1.0, std=1.0)\n",
    "        #self.Ws2 = nn.Parameter(Ws2)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        tokens, lengths = sentence;\n",
    "        #print('len', lengths[0])\n",
    "        lengths = list(torch.unbind(lengths))\n",
    "        tokens = list(torch.unbind(tokens))\n",
    "        A = [] \n",
    "        \n",
    "        batch = []\n",
    "        for i, (t, l) in enumerate(zip(tokens, lengths)):\n",
    "            batch.append( [i, t, l, None] )\n",
    "            \n",
    "        batch = sorted(batch, reverse=True, key=lambda x: x[2]) # sort by lengths\n",
    "        \n",
    "        tokens = torch.stack([x[1] for x in batch])\n",
    "        lengths = torch.stack([x[2] for x in batch])\n",
    "        \n",
    "        # get embedding vectors\n",
    "        embedded = self.embedding(tokens)\n",
    "        \n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True)\n",
    "        \n",
    "        out, (hidden, _) = self.RNN(packed)\n",
    "        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        #hidden = torch.cat([h for h in hidden], dim=1)\n",
    "        #print('hidden', hidden.data.shape)\n",
    "        #for h in out.transpose(0,1):\n",
    "            #A.append(F.softmax(self.Ws2 @ F.tanh(self.Ws1 @ h.t()), dim=1))\n",
    "        #view(num_layers, num_directions, batch, hidden_size)\n",
    "        #print('hidden',hidden.shape)\n",
    "        #print('out',out.shape)   \n",
    "        #a = [ F.softmax(self.Ws2 @ F.tanh(self.Ws1 @ h.t()), dim=1) for h in out ]\n",
    "        # attention distrubution\n",
    "        #A = torch.stack(a)\n",
    "        #print('out',out.transpose(0,1).shape, 'A', A.shape)\n",
    "        A = self.MLP(out).transpose(2,1)\n",
    "        u = A@out\n",
    "        \n",
    "        # add the vectors to the batch list\n",
    "        for i, v in enumerate(u):\n",
    "            batch[i][3] = v\n",
    "            \n",
    "        batch = sorted(batch, key=lambda x: x[0]) # sort by original batch indexes\n",
    "        u = torch.stack([x[3] for x in batch])\n",
    "        u = u.transpose(1,0)\n",
    "        \n",
    "        u = torch.cat([v for v in u], dim=1)\n",
    "        #print('rep u', u.shape)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26530057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04193977fd54b1cabd132900e717e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#net = noteboookmodel2(token_field.vocab, label_field.vocab)\n",
    "#enc = Baseline(token_field.vocab)\n",
    "enc = self_attention_Encoder2(token_field.vocab)\n",
    "net = MLP(token_field.vocab, label_field.vocab, enc)\n",
    "print(sum(p.numel() for p in net.parameters() if p.requires_grad))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimiser = torch.optim.Adam([{'params': net.parameters()}, {'params': enc.parameters()}], lr=1e-3)\n",
    "optimiser = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "# Writer will output to ./runs/ directory by default\n",
    "MODEL_NAME = \"mlp-attention_1_run2\"\n",
    "writer = SummaryWriter('./runs/{}'.format(MODEL_NAME))\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(net, train_iter, criterion, optimiser, epoch, writer)\n",
    "    evaluate(net, dev_iter, criterion, epoch, writer)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_Encoder2_2(nn.Module):\n",
    "    def __init__(self, token_vocab, hidden_size=1024, dimension_a=256, attention_hops=4):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # word embeddings\n",
    "        #self.embedding = nn.Embedding.from_pretrained(token_vocab.vectors)\n",
    "        self.embedding = nn.Embedding(len(token_vocab), 300, padding_idx=token_vocab.stoi['<pad>']).from_pretrained(token_vocab.vectors)\n",
    "\n",
    "        self.RNN = nn.LSTM(token_vocab.vectors.shape[1], hidden_size, bias=True, batch_first=True, bidirectional=True)\n",
    "        #self.RNN_2 = nn.LSTM(token_vocab.vectors.shape[1], hidden_size, bias=True, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.MLP = nn.Sequential(nn.Linear(hidden_size*2, dimension_a), # input layer\n",
    "                                 nn.Tanh(), # hidden layer\n",
    "                                 nn.Linear(dimension_a, attention_hops), # ouput layer\n",
    "                                 nn.Softmax(dim=1)\n",
    "                                )\n",
    "        # alpha weights\n",
    "        #Ws1 = torch.zeros(dimension_a, hidden_size*2)\n",
    "        #nn.init.normal_(Ws1, mean=0.0, std=1.0)\n",
    "        #self.Ws1 = nn.Parameter(Ws1)\n",
    "        \n",
    "        #Ws2 = torch.zeros(attention_hops, dimension_a)\n",
    "        #nn.init.normal_(Ws2, mean=1.0, std=1.0)\n",
    "        #self.Ws2 = nn.Parameter(Ws2)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        tokens, lengths = sentence;\n",
    "        #print('len', lengths[0])\n",
    "        lengths = list(torch.unbind(lengths))\n",
    "        tokens = list(torch.unbind(tokens))\n",
    "        A = [] \n",
    "        \n",
    "        batch = []\n",
    "        for i, (t, l) in enumerate(zip(tokens, lengths)):\n",
    "            batch.append( [i, t, l, None] )\n",
    "            \n",
    "        batch = sorted(batch, reverse=True, key=lambda x: x[2]) # sort by lengths\n",
    "        \n",
    "        tokens = torch.stack([x[1] for x in batch])\n",
    "        lengths = torch.stack([x[2] for x in batch])\n",
    "        \n",
    "        # get embedding vectors\n",
    "        embedded = self.embedding(tokens)\n",
    "        \n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True)\n",
    "        \n",
    "        out, (hidden, _) = self.RNN(packed)\n",
    "        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        #hidden = torch.cat([h for h in hidden], dim=1)\n",
    "        #print('hidden', hidden.data.shape)\n",
    "        #for h in out.transpose(0,1):\n",
    "            #A.append(F.softmax(self.Ws2 @ F.tanh(self.Ws1 @ h.t()), dim=1))\n",
    "        #view(num_layers, num_directions, batch, hidden_size)\n",
    "        #print('hidden',hidden.shape)\n",
    "        #print('out',out.shape)   \n",
    "        #a = [ F.softmax(self.Ws2 @ F.tanh(self.Ws1 @ h.t()), dim=1) for h in out ]\n",
    "        # attention distrubution\n",
    "        #A = torch.stack(a)\n",
    "        #print('out',out.transpose(0,1).shape, 'A', A.shape)\n",
    "        A = self.MLP(out).transpose(2,1)\n",
    "        u = A@out\n",
    "        u = torch.sum(u, 1)\n",
    "        # add the vectors to the batch list\n",
    "        for i, v in enumerate(u):\n",
    "            batch[i][3] = v\n",
    "            \n",
    "        batch = sorted(batch, key=lambda x: x[0]) # sort by original batch indexes\n",
    "        u = torch.stack([x[3] for x in batch])\n",
    "        u = u.transpose(1,0)\n",
    "        \n",
    "        u = torch.cat([v for v in u], dim=1)\n",
    "        #print('rep u', u.shape)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#net = noteboookmodel2(token_field.vocab, label_field.vocab)\n",
    "#enc = Baseline(token_field.vocab)\n",
    "enc = self_attention_Encoder2_2(token_field.vocab)\n",
    "net = MLP(token_field.vocab, label_field.vocab, enc)\n",
    "print(sum(p.numel() for p in net.parameters() if p.requires_grad))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimiser = torch.optim.Adam([{'params': net.parameters()}, {'params': enc.parameters()}], lr=1e-3)\n",
    "optimiser = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "# Writer will output to ./runs/ directory by default\n",
    "MODEL_NAME = \"mlp-attention_1_max\"\n",
    "writer = SummaryWriter('./runs/{}'.format(MODEL_NAME))\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(net, train_iter, criterion, optimiser, epoch, writer)\n",
    "    evaluate(net, dev_iter, criterion, epoch, writer)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard.notebook.start('--logdir ./runs')\n",
    "#http://www.erogol.com/use-tensorboard-pytorch/#viewSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
